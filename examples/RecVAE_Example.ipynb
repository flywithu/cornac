{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flywithu/cornac/blob/master/examples/RecVAE_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y32qGVxx2sv"
      },
      "outputs": [],
      "source": [
        "!pip install cornac==1.17 bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiGFdw8Ax4pU"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rvhQQ55x7DZ"
      },
      "outputs": [],
      "source": [
        "FILE_PREFIX=\".\"\n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  FILE_PREFIX=\"/content/drive/MyDrive/mycornac\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# sys.path.insert(0,'/content/drive/MyDrive/daicon/msr')\n",
        "if \".\" not in sys.path:\n",
        "  sys.path.insert(0,FILE_PREFIX)"
      ],
      "metadata": {
        "id": "pxLjuHpQSgbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/flywithu/RecVAE/master/utils.py -O utils.py"
      ],
      "metadata": {
        "id": "SW6W9_6SJ3ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "h19rR_xFSt8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (sys.path)"
      ],
      "metadata": {
        "id": "jj_xd6ODSq-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_train_data(csv_file, n_items, n_users, global_indexing=False):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "\n",
        "    n_users = n_users if global_indexing else tp['uid'].max() + 1\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                             (rows, cols)), dtype='float64',\n",
        "                             shape=(n_users, n_items))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_tr_te_data(csv_file_tr, csv_file_te, n_items, n_users, global_indexing=False):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    if global_indexing:\n",
        "        start_idx = 0\n",
        "        end_idx = len(unique_uid) - 1\n",
        "    else:\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te\n",
        "\n",
        "\n",
        "def get_data(dataset, global_indexing=False):\n",
        "    unique_sid = list()\n",
        "    with open(os.path.join(dataset, 'unique_sid.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            unique_sid.append(line.strip())\n",
        "\n",
        "    unique_uid = list()\n",
        "    with open(os.path.join(dataset, 'unique_uid.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            unique_uid.append(line.strip())\n",
        "\n",
        "    n_items = len(unique_sid)\n",
        "    n_users = len(unique_uid)\n",
        "\n",
        "    train_data = load_train_data(os.path.join(dataset, 'train.csv'), n_items, n_users, global_indexing=global_indexing)\n",
        "\n",
        "\n",
        "    vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(dataset, 'validation_tr.csv'),\n",
        "                                               os.path.join(dataset, 'validation_te.csv'),\n",
        "                                               n_items, n_users,\n",
        "                                               global_indexing=global_indexing)\n",
        "\n",
        "    test_data_tr, test_data_te = load_tr_te_data(os.path.join(dataset, 'test_tr.csv'),\n",
        "                                                 os.path.join(dataset, 'test_te.csv'),\n",
        "                                                 n_items, n_users,\n",
        "                                                 global_indexing=global_indexing)\n",
        "\n",
        "    data = train_data, vad_data_tr, vad_data_te, test_data_tr, test_data_te\n",
        "    data = (x.astype('float32') for x in data)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def ndcg(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    normalized discounted cumulative gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "\n",
        "    # iszero = torch.sum(torch.isnan(X_pred)).item()\n",
        "    # iszero = np.sum(np.isnan(X_pred))\n",
        "    # print(f\"z ::: {iszero}\")\n",
        "    # import time\n",
        "    # time.sleep(10)\n",
        "\n",
        "\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
        "    # topk predicted score\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "    # build the discount template\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def recall(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    #모두 0으로 셋팅\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.maximum(np.minimum(k, X_true_binary.sum(axis=1)),1)\n",
        "    return recall\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T8_rI2BA08HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x.mul(torch.sigmoid(x))\n",
        "\n",
        "def log_norm_pdf(x, mu, logvar):\n",
        "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
        "\n",
        "\n",
        "class CompositePrior(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
        "        super(CompositePrior, self).__init__()\n",
        "\n",
        "        self.mixture_weights = mixture_weights\n",
        "\n",
        "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.mu_prior.data.fill_(0)\n",
        "\n",
        "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_prior.data.fill_(0)\n",
        "\n",
        "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_uniform_prior.data.fill_(10)\n",
        "\n",
        "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.encoder_old.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        post_mu, post_logvar = self.encoder_old(x, 0)\n",
        "\n",
        "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
        "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
        "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
        "\n",
        "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
        "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
        "\n",
        "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
        "\n",
        "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, dropout_rate):\n",
        "        x = F.normalize(x)\n",
        "        x = F.dropout(x, dropout_rate, training=self.training)\n",
        "\n",
        "        h1 = self.ln1(swish(self.fc1(x)))\n",
        "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
        "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
        "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
        "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
        "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
        "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5*logvar)\n",
        "            eps = torch.zeros_like(std).normal_(mean=0, std=0.01)\n",
        "            return mu + eps * std\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
        "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_pred = self.decoder(z)\n",
        "\n",
        "        # iszero = torch.sum(torch.isnan(x_pred)).item()\n",
        "        # print(f\"z ::: {iszero}\")\n",
        "        # import time\n",
        "        # time.sleep(10)\n",
        "\n",
        "\n",
        "        if calculate_loss:\n",
        "            if gamma:\n",
        "                norm = user_ratings.sum(dim=-1)\n",
        "                kl_weight = gamma * norm\n",
        "            elif beta:\n",
        "                kl_weight = beta\n",
        "\n",
        "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
        "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
        "            negative_elbo = -(mll - kld)\n",
        "\n",
        "\n",
        "\n",
        "            return (mll, kld), negative_elbo\n",
        "\n",
        "        else:\n",
        "            return x_pred\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
      ],
      "metadata": {
        "id": "vfIazOiuSMUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "import bottleneck as bn\n",
        "\n",
        "\n",
        "# import argparse\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='/content/drive/MyDrive/mycornac')\n",
        "# parser.add_argument('--hidden-dim', type=int, default=600)\n",
        "# parser.add_argument('--latent-dim', type=int, default=200)\n",
        "# parser.add_argument('--batch-size', type=int, default=500)\n",
        "# parser.add_argument('--beta', type=float, default=None)\n",
        "# parser.add_argument('--gamma', type=float, default=0.005)\n",
        "# parser.add_argument('--lr', type=float, default=5e-4)\n",
        "# parser.add_argument('--n-epochs', type=int, default=50)\n",
        "# parser.add_argument('--n-enc_epochs', type=int, default=3)\n",
        "# parser.add_argument('--n-dec_epochs', type=int, default=1)\n",
        "# parser.add_argument('--not-alternating', type=bool, default=False)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "seed = 1337\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = get_data('/content/drive/MyDrive/mycornac/data/20m/pro_sg')\n",
        "train_data, valid_in_data, valid_out_data, test_in_data, test_out_data = data"
      ],
      "metadata": {
        "id": "wqtcDi_ufk-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 600\n",
        "latent_dim = 200\n",
        "batch_size = 500\n",
        "beta = None\n",
        "gamma = 0.005\n",
        "lr = 5e-4\n",
        "n_epochs = 2\n",
        "n_enc_epochs = 3\n",
        "n_dec_epochs = 1\n",
        "not_alternating = False\n",
        "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
        "    assert 0 < samples_perc_per_epoch <= 1\n",
        "\n",
        "    total_samples = data_in.shape[0]\n",
        "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
        "\n",
        "    if shuffle:\n",
        "        idxlist = np.arange(total_samples)\n",
        "        np.random.shuffle(idxlist)\n",
        "        idxlist = idxlist[:samples_per_epoch]\n",
        "    else:\n",
        "        idxlist = np.arange(samples_per_epoch)\n",
        "\n",
        "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
        "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
        "        idx = idxlist[st_idx:end_idx]\n",
        "\n",
        "        yield Batch(device, idx, data_in, data_out)\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    def __init__(self, device, idx, data_in, data_out=None):\n",
        "        self._device = device\n",
        "        self._idx = idx\n",
        "        self._data_in = data_in\n",
        "        self._data_out = data_out\n",
        "\n",
        "    def get_idx(self):\n",
        "        return self._idx\n",
        "\n",
        "    def get_idx_to_dev(self):\n",
        "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
        "\n",
        "    def get_ratings(self, is_out=False):\n",
        "        data = self._data_out if is_out else self._data_in\n",
        "        return data[self._idx]\n",
        "\n",
        "    def get_ratings_to_dev(self, is_out=False):\n",
        "        return torch.Tensor(\n",
        "            self.get_ratings(is_out).toarray()\n",
        "        ).to(self._device)\n",
        "\n",
        "\n",
        "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
        "    metrics = deepcopy(metrics)\n",
        "    model.eval()\n",
        "\n",
        "    for m in metrics:\n",
        "        m['score'] = []\n",
        "\n",
        "    for batch in generate(batch_size=batch_size,\n",
        "                          device=device,\n",
        "                          data_in=data_in,\n",
        "                          data_out=data_out,\n",
        "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
        "                         ):\n",
        "\n",
        "        ratings_in = batch.get_ratings_to_dev()\n",
        "        ratings_out = batch.get_ratings(is_out=True)\n",
        "\n",
        "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
        "\n",
        "        if not (data_in is data_out):\n",
        "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
        "\n",
        "        for m in metrics:\n",
        "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
        "\n",
        "    for m in metrics:\n",
        "        m['score'] = np.concatenate(m['score']).mean()\n",
        "\n",
        "    return [x['score'] for x in metrics]\n",
        "\n",
        "\n",
        "def run(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f'epoch {epoch}')\n",
        "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
        "            ratings = batch.get_ratings_to_dev()\n",
        "\n",
        "            for optimizer in opts:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            _, loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
        "            loss.backward()\n",
        "\n",
        "            for optimizer in opts:\n",
        "                optimizer.step()\n",
        "\n",
        "\n",
        "model_kwargs = {\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'latent_dim': latent_dim,\n",
        "    'input_dim': train_data.shape[1]\n",
        "}\n",
        "metrics = [{'metric': ndcg, 'k': 100}]\n",
        "\n",
        "best_ndcg = -np.inf\n",
        "train_scores, valid_scores = [], []\n",
        "\n",
        "model = VAE(**model_kwargs).to(device)\n",
        "model_best = VAE(**model_kwargs).to(device)\n",
        "\n",
        "learning_kwargs = {\n",
        "    'model': model,\n",
        "    'train_data': train_data,\n",
        "    'batch_size': batch_size,\n",
        "    'beta': beta,\n",
        "    'gamma': gamma\n",
        "}\n",
        "\n",
        "decoder_params = set(model.decoder.parameters())\n",
        "encoder_params = set(model.encoder.parameters())\n",
        "\n",
        "optimizer_encoder = optim.Adam(encoder_params, lr=lr)\n",
        "optimizer_decoder = optim.Adam(decoder_params, lr=lr)\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    if not_alternating:\n",
        "        run(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=0.5, **learning_kwargs)\n",
        "    else:\n",
        "        run(opts=[optimizer_encoder], n_epochs=n_enc_epochs, dropout_rate=0.5, **learning_kwargs)\n",
        "        model.update_prior()\n",
        "        run(opts=[optimizer_decoder], n_epochs=n_dec_epochs, dropout_rate=0, **learning_kwargs)\n",
        "\n",
        "    train_scores.append(\n",
        "        evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
        "    )\n",
        "    valid_scores.append(\n",
        "        evaluate(model, valid_in_data, valid_out_data, metrics, 1)[0]\n",
        "    )\n",
        "\n",
        "    if valid_scores[-1] > best_ndcg:\n",
        "        best_ndcg = valid_scores[-1]\n",
        "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
        "\n",
        "\n",
        "    print(f'epoch {epoch} | valid ndcg@100: {valid_scores[-1]:.4f} | ' +\n",
        "          f'best valid: {best_ndcg:.4f} | train ndcg@100: {train_scores[-1]:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "test_metrics = [{'metric': ndcg, 'k': 100}, {'metric': recall, 'k': 20}, {'metric': recall, 'k': 50}]\n",
        "\n",
        "final_scores = evaluate(model_best, test_in_data, test_out_data, test_metrics)\n",
        "\n",
        "for metric, score in zip(test_metrics, final_scores):\n",
        "    print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")"
      ],
      "metadata": {
        "id": "MiXmbjgXRtid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.exit(1)"
      ],
      "metadata": {
        "id": "k3yPf_a2Ry3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install recbole\n"
      ],
      "metadata": {
        "id": "oGGS7pPOz9Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_recbole.py --model=recvae\n"
      ],
      "metadata": {
        "id": "HJSfJBH10F6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAHi57eR0Q1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}