{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flywithu/cornac/blob/master/examples/RecVAE_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "2Y32qGVxx2sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b229c1f-8519-4ed4-d676-7122cf5b565d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cornac==1.17 in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cornac==1.17) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cornac==1.17) (1.11.3)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.10/dist-packages (from cornac==1.17) (4.66.1)\n",
            "Requirement already satisfied: powerlaw in /usr/local/lib/python3.10/dist-packages (from cornac==1.17) (1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from powerlaw->cornac==1.17) (3.7.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.10/dist-packages (from powerlaw->cornac==1.17) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac==1.17) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->powerlaw->cornac==1.17) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cornac==1.17 bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "iiGFdw8Ax4pU"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "_rvhQQ55x7DZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f1390a-7c64-40d0-f431-b8068da4ab7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "FILE_PREFIX=\".\"\n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  FILE_PREFIX=\"/content/drive/MyDrive/mycornac\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# sys.path.insert(0,'/content/drive/MyDrive/daicon/msr')\n",
        "if \".\" not in sys.path:\n",
        "  sys.path.insert(0,FILE_PREFIX)"
      ],
      "metadata": {
        "id": "pxLjuHpQSgbi"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/flywithu/RecVAE/master/utils.py -O utils.py"
      ],
      "metadata": {
        "id": "SW6W9_6SJ3ZH"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "h19rR_xFSt8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba0a380-e632-4a9d-e8e4-7d973bfa2815"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (sys.path)"
      ],
      "metadata": {
        "id": "jj_xd6ODSq-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b7546b-4da1-4295-fffe-825dd4f2a117"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/mycornac', '/content/drive/MyDrive/mycornac', '/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_train_data(csv_file, n_items, n_users, global_indexing=False):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "\n",
        "    n_users = n_users if global_indexing else tp['uid'].max() + 1\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                             (rows, cols)), dtype='float64',\n",
        "                             shape=(n_users, n_items))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_tr_te_data(csv_file_tr, csv_file_te, n_items, n_users, global_indexing=False):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    if global_indexing:\n",
        "        start_idx = 0\n",
        "        end_idx = len(unique_uid) - 1\n",
        "    else:\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te\n",
        "\n",
        "\n",
        "def get_data(dataset, global_indexing=False):\n",
        "    unique_sid = list()\n",
        "    with open(os.path.join(dataset, 'unique_sid.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            unique_sid.append(line.strip())\n",
        "\n",
        "    unique_uid = list()\n",
        "    with open(os.path.join(dataset, 'unique_uid.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            unique_uid.append(line.strip())\n",
        "\n",
        "    n_items = len(unique_sid)\n",
        "    n_users = len(unique_uid)\n",
        "\n",
        "    train_data = load_train_data(os.path.join(dataset, 'train.csv'), n_items, n_users, global_indexing=global_indexing)\n",
        "\n",
        "\n",
        "    vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(dataset, 'validation_tr.csv'),\n",
        "                                               os.path.join(dataset, 'validation_te.csv'),\n",
        "                                               n_items, n_users,\n",
        "                                               global_indexing=global_indexing)\n",
        "\n",
        "    test_data_tr, test_data_te = load_tr_te_data(os.path.join(dataset, 'test_tr.csv'),\n",
        "                                                 os.path.join(dataset, 'test_te.csv'),\n",
        "                                                 n_items, n_users,\n",
        "                                                 global_indexing=global_indexing)\n",
        "\n",
        "    data = train_data, vad_data_tr, vad_data_te, test_data_tr, test_data_te\n",
        "    data = (x.astype('float32') for x in data)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def ndcg(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    normalized discounted cumulative gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "\n",
        "    #X_pred : array\n",
        "    #heldout_batch : tuple\n",
        "    # print(f\"Xpred :{len(X_pred)} \")\n",
        "    # print(f\"heldout_batch: {len(heldout_batch)} \")\n",
        "    # print(f\"SHAPE : {X_pred.shape}\")\n",
        "    # print(f\"Xpred:0 : {np.isnan(X_pred).sum()}\")\n",
        "    # print(f\"Xpred:inf : {np.isinf(X_pred).sum()}\")\n",
        "\n",
        "    # print(f\"heldout_batch:0 : {heldout_batch.size - heldout_batch.getnnz()}\")\n",
        "\n",
        "    batch_users = X_pred.shape[0]\n",
        "    # print(\"BN\")\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    # print(f\"idx_topk_part {idx_topk_part.shape}\")\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
        "    # topk predicted score\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "    # build the discount template\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "\n",
        "    # print(f\"DCG: {DCG.shape}\")\n",
        "    # print(f\"DCG.Nan: {np.isnan(DCG).sum()}\")\n",
        "    # print(f\"DCG.INf: {np.isinf(DCG).sum()}\")\n",
        "    # print(f\"DCG.0: {np.sum(DCG==0)}\")\n",
        "\n",
        "    # print(f\"IDCG: {IDCG.shape}\")\n",
        "    # print(f\"IDCG.Nan: {np.isnan(IDCG).sum()}\")\n",
        "    # print(f\"IDCG.INf: {np.isinf(IDCG).sum()}\")\n",
        "    # print(f\"IDCG.0: {np.sum(IDCG==0)}\")\n",
        "\n",
        "\n",
        "    value = DCG / IDCG\n",
        "\n",
        "    # print(f\"value: {value.shape}\")\n",
        "    # print(f\"value.Nan: {np.isnan(value).sum()}\")\n",
        "    # print(f\"value.INf: {np.isinf(value).sum()}\")\n",
        "\n",
        "    value = np.nan_to_num(value, nan=0.0)\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "def recall(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "\n",
        "    # print(f\"value: {recall.shape}\")\n",
        "    # print(f\"value.Nan: {np.isnan(recall).sum()}\")\n",
        "    # print(f\"value.INf: {np.isinf(recall).sum()}\")\n",
        "\n",
        "    recall = np.nan_to_num(recall, nan=0.0)\n",
        "\n",
        "    return recall\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T8_rI2BA08HI"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x.mul(torch.sigmoid(x))\n",
        "\n",
        "def log_norm_pdf(x, mu, logvar):\n",
        "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
        "\n",
        "\n",
        "class CompositePrior(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
        "        super(CompositePrior, self).__init__()\n",
        "\n",
        "        self.mixture_weights = mixture_weights\n",
        "\n",
        "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.mu_prior.data.fill_(0)\n",
        "\n",
        "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_prior.data.fill_(0)\n",
        "\n",
        "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_uniform_prior.data.fill_(10)\n",
        "\n",
        "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.encoder_old.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        post_mu, post_logvar = self.encoder_old(x, 0)\n",
        "\n",
        "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
        "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
        "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
        "\n",
        "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
        "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
        "\n",
        "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
        "\n",
        "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, dropout_rate):\n",
        "        x = F.normalize(x)\n",
        "        x = F.dropout(x, dropout_rate, training=self.training)\n",
        "\n",
        "        h1 = self.ln1(swish(self.fc1(x)))\n",
        "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
        "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
        "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
        "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
        "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
        "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5*logvar)\n",
        "            eps = torch.zeros_like(std).normal_(mean=0, std=0.01)\n",
        "            return mu + eps * std\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
        "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_pred = self.decoder(z)\n",
        "\n",
        "        # iszero = torch.sum(torch.isnan(x_pred)).item()\n",
        "        # print(f\"z ::: {iszero}\")\n",
        "        # import time\n",
        "        # time.sleep(10)\n",
        "\n",
        "\n",
        "        if calculate_loss:\n",
        "            if gamma:\n",
        "                norm = user_ratings.sum(dim=-1)\n",
        "                kl_weight = gamma * norm\n",
        "            elif beta:\n",
        "                kl_weight = beta\n",
        "\n",
        "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
        "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
        "            negative_elbo = -(mll - kld)\n",
        "\n",
        "\n",
        "\n",
        "            return (mll, kld), negative_elbo\n",
        "\n",
        "        else:\n",
        "            return x_pred\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
      ],
      "metadata": {
        "id": "vfIazOiuSMUU"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "import bottleneck as bn\n",
        "\n",
        "\n",
        "# import argparse\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='/content/drive/MyDrive/mycornac')\n",
        "# parser.add_argument('--hidden-dim', type=int, default=600)\n",
        "# parser.add_argument('--latent-dim', type=int, default=200)\n",
        "# parser.add_argument('--batch-size', type=int, default=500)\n",
        "# parser.add_argument('--beta', type=float, default=None)\n",
        "# parser.add_argument('--gamma', type=float, default=0.005)\n",
        "# parser.add_argument('--lr', type=float, default=5e-4)\n",
        "# parser.add_argument('--n-epochs', type=int, default=50)\n",
        "# parser.add_argument('--n-enc_epochs', type=int, default=3)\n",
        "# parser.add_argument('--n-dec_epochs', type=int, default=1)\n",
        "# parser.add_argument('--not-alternating', type=bool, default=False)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "seed = 1337\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = get_data('/content/drive/MyDrive/mycornac/data/20m/pro_sg')\n",
        "train_data, valid_in_data, valid_out_data, test_in_data, test_out_data = data\n",
        "train_data = train_data[:-477]"
      ],
      "metadata": {
        "id": "wqtcDi_ufk-S"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWCbyJlcH8Qu",
        "outputId": "9f4a63dd-6e03-4f48-f720-9ac6622cd217"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(116000, 20101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_in_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLPB3X9LINtG",
        "outputId": "02abffd9-e64e-4836-83ce-689e07c76cd9"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 20101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_out_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agmupgE9IQ-W",
        "outputId": "c5666f66-b5fe-4f91-dc51-83a2f49e9358"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 20101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_in_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbNUd71JIQu9",
        "outputId": "e9140b9e-27ae-4805-81c1-f1e8ae19b4e7"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 20101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_out_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54kD3_ZLIVzs",
        "outputId": "3c5dacb4-dd21-480c-fcc4-8acd8cc847ae"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 20101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 600\n",
        "latent_dim = 200\n",
        "batch_size = 500\n",
        "beta = None\n",
        "gamma = 0.005\n",
        "lr = 5e-4\n",
        "n_epochs = 50\n",
        "n_enc_epochs = 3\n",
        "n_dec_epochs = 1\n",
        "not_alternating = False\n",
        "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
        "    assert 0 < samples_perc_per_epoch <= 1\n",
        "\n",
        "    total_samples = data_in.shape[0]\n",
        "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
        "\n",
        "    if shuffle:\n",
        "        idxlist = np.arange(total_samples)\n",
        "        np.random.shuffle(idxlist)\n",
        "        idxlist = idxlist[:samples_per_epoch]\n",
        "    else:\n",
        "        idxlist = np.arange(samples_per_epoch)\n",
        "\n",
        "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
        "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
        "        idx = idxlist[st_idx:end_idx]\n",
        "\n",
        "        # idx_len = len(idx)\n",
        "        # print(f\"end_idx:{end_idx}\")\n",
        "        # if idx_len != 500:\n",
        "        #   print(f\"idx:{len(idx)}\")\n",
        "        #   break\n",
        "\n",
        "        yield Batch(device, idx, data_in, data_out)\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    def __init__(self, device, idx, data_in, data_out=None):\n",
        "        self._device = device\n",
        "        self._idx = idx\n",
        "        self._data_in = data_in\n",
        "        self._data_out = data_out\n",
        "\n",
        "    def get_idx(self):\n",
        "        return self._idx\n",
        "\n",
        "    def get_idx_to_dev(self):\n",
        "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
        "\n",
        "    def get_ratings(self, is_out=False):\n",
        "        data = self._data_out if is_out else self._data_in\n",
        "        return data[self._idx]\n",
        "\n",
        "    def get_ratings_to_dev(self, is_out=False):\n",
        "        return torch.Tensor(\n",
        "            self.get_ratings(is_out).toarray()\n",
        "        ).to(self._device)\n",
        "\n",
        "\n",
        "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
        "    metrics = deepcopy(metrics)\n",
        "    model.eval()\n",
        "\n",
        "    for m in metrics:\n",
        "        m['score'] = []\n",
        "\n",
        "    for batch in generate(batch_size=batch_size,\n",
        "                          device=device,\n",
        "                          data_in=data_in,\n",
        "                          data_out=data_out,\n",
        "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
        "                         ):\n",
        "\n",
        "        ratings_in = batch.get_ratings_to_dev()\n",
        "        ratings_out = batch.get_ratings(is_out=True)\n",
        "\n",
        "\n",
        "\n",
        "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "        if not (data_in is data_out):\n",
        "            # print(\"Pred : INF\")\n",
        "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
        "\n",
        "        for m in metrics:\n",
        "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
        "\n",
        "    for m in metrics:\n",
        "        m['score'] = np.concatenate(m['score']).mean()\n",
        "\n",
        "    return [x['score'] for x in metrics]\n",
        "\n",
        "\n",
        "def run(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        # print(f'epoch {epoch}')\n",
        "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
        "            ratings = batch.get_ratings_to_dev()\n",
        "            for optimizer in opts:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            _, loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
        "            loss.backward()\n",
        "\n",
        "            for optimizer in opts:\n",
        "                optimizer.step()\n",
        "\n",
        "\n",
        "model_kwargs = {\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'latent_dim': latent_dim,\n",
        "    'input_dim': train_data.shape[1]\n",
        "}\n",
        "metrics = [{'metric': ndcg, 'k': 100}]\n",
        "\n",
        "best_ndcg = -np.inf\n",
        "train_scores, valid_scores = [], []\n",
        "\n",
        "model = VAE(**model_kwargs).to(device)\n",
        "model_best = VAE(**model_kwargs).to(device)\n",
        "\n",
        "learning_kwargs = {\n",
        "    'model': model,\n",
        "    'train_data': train_data,\n",
        "    'batch_size': batch_size,\n",
        "    'beta': beta,\n",
        "    'gamma': gamma\n",
        "}\n",
        "\n",
        "decoder_params = set(model.decoder.parameters())\n",
        "encoder_params = set(model.encoder.parameters())\n",
        "\n",
        "optimizer_encoder = optim.Adam(encoder_params, lr=lr)\n",
        "optimizer_decoder = optim.Adam(decoder_params, lr=lr)\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    if not_alternating:\n",
        "        run(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=0.5, **learning_kwargs)\n",
        "    else:\n",
        "        run(opts=[optimizer_encoder], n_epochs=n_enc_epochs, dropout_rate=0.5, **learning_kwargs)\n",
        "        model.update_prior()\n",
        "        run(opts=[optimizer_decoder], n_epochs=n_dec_epochs, dropout_rate=0, **learning_kwargs)\n",
        "\n",
        "    # print(\"TrainScore\")\n",
        "    train_scores.append(\n",
        "        evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
        "    )\n",
        "    # print(\"ValidScore\")\n",
        "\n",
        "    valid_scores.append(\n",
        "        evaluate(model, valid_in_data, valid_out_data, metrics, 1)[0]\n",
        "    )\n",
        "\n",
        "    if valid_scores[-1] > best_ndcg:\n",
        "        best_ndcg = valid_scores[-1]\n",
        "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
        "\n",
        "\n",
        "    print(f'epoch {epoch} | valid ndcg@100: {valid_scores[-1]:.4f} | ' +\n",
        "          f'best valid: {best_ndcg:.4f} | train ndcg@100: {train_scores[-1]:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "test_metrics = [{'metric': ndcg, 'k': 100}, {'metric': recall, 'k': 20}, {'metric': recall, 'k': 50}]\n",
        "\n",
        "final_scores = evaluate(model_best, test_in_data, test_out_data, test_metrics)\n",
        "\n",
        "for metric, score in zip(test_metrics, final_scores):\n",
        "    print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")"
      ],
      "metadata": {
        "id": "MiXmbjgXRtid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca18b235-8d1f-426e-8af2-ff756e4ba7e9"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-260-341dc376f668>:112: RuntimeWarning: invalid value encountered in divide\n",
            "  value = DCG / IDCG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 | valid ndcg@100: 0.3092 | best valid: 0.3092 | train ndcg@100: 0.6656\n",
            "epoch 1 | valid ndcg@100: 0.3784 | best valid: 0.3784 | train ndcg@100: 0.7431\n",
            "epoch 2 | valid ndcg@100: 0.3944 | best valid: 0.3944 | train ndcg@100: 0.7557\n",
            "epoch 3 | valid ndcg@100: 0.4015 | best valid: 0.4015 | train ndcg@100: 0.7668\n",
            "epoch 4 | valid ndcg@100: 0.4060 | best valid: 0.4060 | train ndcg@100: 0.7722\n",
            "epoch 5 | valid ndcg@100: 0.4079 | best valid: 0.4079 | train ndcg@100: 0.7743\n",
            "epoch 6 | valid ndcg@100: 0.4114 | best valid: 0.4114 | train ndcg@100: 0.7814\n",
            "epoch 7 | valid ndcg@100: 0.4124 | best valid: 0.4124 | train ndcg@100: 0.7836\n",
            "epoch 8 | valid ndcg@100: 0.4128 | best valid: 0.4128 | train ndcg@100: 0.7833\n",
            "epoch 9 | valid ndcg@100: 0.4135 | best valid: 0.4135 | train ndcg@100: 0.7842\n",
            "epoch 10 | valid ndcg@100: 0.4158 | best valid: 0.4158 | train ndcg@100: 0.7893\n",
            "epoch 11 | valid ndcg@100: 0.4159 | best valid: 0.4159 | train ndcg@100: 0.7908\n",
            "epoch 12 | valid ndcg@100: 0.4170 | best valid: 0.4170 | train ndcg@100: 0.7915\n",
            "epoch 13 | valid ndcg@100: 0.4170 | best valid: 0.4170 | train ndcg@100: 0.7940\n",
            "epoch 14 | valid ndcg@100: 0.4175 | best valid: 0.4175 | train ndcg@100: 0.7958\n",
            "epoch 15 | valid ndcg@100: 0.4185 | best valid: 0.4185 | train ndcg@100: 0.7960\n",
            "epoch 16 | valid ndcg@100: 0.4193 | best valid: 0.4193 | train ndcg@100: 0.7980\n",
            "epoch 17 | valid ndcg@100: 0.4188 | best valid: 0.4193 | train ndcg@100: 0.7976\n",
            "epoch 18 | valid ndcg@100: 0.4194 | best valid: 0.4194 | train ndcg@100: 0.7991\n",
            "epoch 19 | valid ndcg@100: 0.4194 | best valid: 0.4194 | train ndcg@100: 0.7995\n",
            "epoch 20 | valid ndcg@100: 0.4201 | best valid: 0.4201 | train ndcg@100: 0.8008\n",
            "epoch 21 | valid ndcg@100: 0.4206 | best valid: 0.4206 | train ndcg@100: 0.8008\n",
            "epoch 22 | valid ndcg@100: 0.4201 | best valid: 0.4206 | train ndcg@100: 0.8025\n",
            "epoch 23 | valid ndcg@100: 0.4208 | best valid: 0.4208 | train ndcg@100: 0.8028\n",
            "epoch 24 | valid ndcg@100: 0.4213 | best valid: 0.4213 | train ndcg@100: 0.8035\n",
            "epoch 25 | valid ndcg@100: 0.4205 | best valid: 0.4213 | train ndcg@100: 0.8032\n",
            "epoch 26 | valid ndcg@100: 0.4212 | best valid: 0.4213 | train ndcg@100: 0.8034\n",
            "epoch 27 | valid ndcg@100: 0.4216 | best valid: 0.4216 | train ndcg@100: 0.8031\n",
            "epoch 28 | valid ndcg@100: 0.4223 | best valid: 0.4223 | train ndcg@100: 0.8052\n",
            "epoch 29 | valid ndcg@100: 0.4223 | best valid: 0.4223 | train ndcg@100: 0.8032\n",
            "epoch 30 | valid ndcg@100: 0.4220 | best valid: 0.4223 | train ndcg@100: 0.8059\n",
            "epoch 31 | valid ndcg@100: 0.4221 | best valid: 0.4223 | train ndcg@100: 0.8060\n",
            "epoch 32 | valid ndcg@100: 0.4218 | best valid: 0.4223 | train ndcg@100: 0.8052\n",
            "epoch 33 | valid ndcg@100: 0.4223 | best valid: 0.4223 | train ndcg@100: 0.8063\n",
            "epoch 34 | valid ndcg@100: 0.4227 | best valid: 0.4227 | train ndcg@100: 0.8081\n",
            "epoch 35 | valid ndcg@100: 0.4228 | best valid: 0.4228 | train ndcg@100: 0.8087\n",
            "epoch 36 | valid ndcg@100: 0.4236 | best valid: 0.4236 | train ndcg@100: 0.8084\n",
            "epoch 37 | valid ndcg@100: 0.4234 | best valid: 0.4236 | train ndcg@100: 0.8081\n",
            "epoch 38 | valid ndcg@100: 0.4237 | best valid: 0.4237 | train ndcg@100: 0.8096\n",
            "epoch 39 | valid ndcg@100: 0.4236 | best valid: 0.4237 | train ndcg@100: 0.8097\n",
            "epoch 40 | valid ndcg@100: 0.4232 | best valid: 0.4237 | train ndcg@100: 0.8079\n",
            "epoch 41 | valid ndcg@100: 0.4238 | best valid: 0.4238 | train ndcg@100: 0.8101\n",
            "epoch 42 | valid ndcg@100: 0.4240 | best valid: 0.4240 | train ndcg@100: 0.8090\n",
            "epoch 43 | valid ndcg@100: 0.4236 | best valid: 0.4240 | train ndcg@100: 0.8107\n",
            "epoch 44 | valid ndcg@100: 0.4237 | best valid: 0.4240 | train ndcg@100: 0.8095\n",
            "epoch 45 | valid ndcg@100: 0.4235 | best valid: 0.4240 | train ndcg@100: 0.8106\n",
            "epoch 46 | valid ndcg@100: 0.4237 | best valid: 0.4240 | train ndcg@100: 0.8097\n",
            "epoch 47 | valid ndcg@100: 0.4240 | best valid: 0.4240 | train ndcg@100: 0.8111\n",
            "epoch 48 | valid ndcg@100: 0.4243 | best valid: 0.4243 | train ndcg@100: 0.8097\n",
            "epoch 49 | valid ndcg@100: 0.4242 | best valid: 0.4243 | train ndcg@100: 0.8115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-260-341dc376f668>:133: RuntimeWarning: invalid value encountered in divide\n",
            "  recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ndcg@100:\t0.4282\n",
            "recall@20:\t0.4009\n",
            "recall@50:\t0.5322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAHi57eR0Q1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}